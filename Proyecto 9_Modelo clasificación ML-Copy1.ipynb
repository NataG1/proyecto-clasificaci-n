{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Planes de Megaline - Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerias\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls  minutes  messages   mb_used  is_ultra\n",
      "0   40.0   311.90      83.0  19915.42         0\n",
      "1   85.0   516.75      56.0  22696.96         0\n",
      "2   77.0   467.66      86.0  21060.45         0\n",
      "3  106.0   745.53      81.0   8437.39         1\n",
      "4   66.0   418.74       1.0  14502.75         0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n",
      "None\n",
      "0    2229\n",
      "1     985\n",
      "Name: is_ultra, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv('/datasets/users_behavior.csv')\n",
    "\n",
    "# Examinar los datos\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df['is_ultra'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que no hay valores nulos, y la mayor parte de usuarios tienen como plan actual el Smart (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentar los datos\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=54321)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=54321)\n",
    "\n",
    "# Extraer características y objetivos\n",
    "features_train = train_df.drop('is_ultra', axis=1)\n",
    "target_train = train_df['is_ultra']\n",
    "\n",
    "features_valid = valid_df.drop('is_ultra', axis=1)\n",
    "target_valid = valid_df['is_ultra']\n",
    "\n",
    "features_test = test_df.drop('is_ultra', axis=1)\n",
    "target_test = test_df['is_ultra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor exactitud del Árbol de Decisión: 0.78\n",
      "Mejores hiperparámetros del Árbol de Decisión: {'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "Mejor exactitud del Bosque Aleatorio: 0.80\n",
      "Mejores hiperparámetros del Bosque Aleatorio: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2}\n",
      "Mejor exactitud de la Regresión Logística: 0.68\n",
      "Mejores hiperparámetros de la Regresión Logística: {'C': 0.1, 'solver': 'liblinear', 'max_iter': 100}\n"
     ]
    }
   ],
   "source": [
    "# Modelo 1: Árbol de decisión\n",
    "# Ajustar hiperparámetros: max_depth, min_samples_split, min_samples_leaf\n",
    "best_tree_accuracy = 0\n",
    "best_tree_params = {}\n",
    "\n",
    "for max_depth in range(1, 11):  # Probar profundidades de 1 a 10\n",
    "    for min_samples_split in [2, 5, 10]:  # Probar diferentes divisiones mínimas\n",
    "        for min_samples_leaf in [1, 2, 4]:  # Probar diferentes hojas mínimas\n",
    "            tree_model = DecisionTreeClassifier(\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                random_state=54321\n",
    "            )\n",
    "            tree_model.fit(features_train, target_train)\n",
    "            tree_predictions = tree_model.predict(features_valid)\n",
    "            tree_accuracy = accuracy_score(target_valid, tree_predictions)\n",
    "            \n",
    "            # Guardar la mejor configuración\n",
    "            if tree_accuracy > best_tree_accuracy:\n",
    "                best_tree_accuracy = tree_accuracy\n",
    "                best_tree_params = {\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf\n",
    "                }\n",
    "\n",
    "print(f\"Mejor exactitud del Árbol de Decisión: {best_tree_accuracy:.2f}\")\n",
    "print(f\"Mejores hiperparámetros del Árbol de Decisión: {best_tree_params}\")\n",
    "\n",
    "# Modelo 2: Bosque aleatorio\n",
    "# Ajustar hiperparámetros: n_estimators, max_depth, min_samples_split\n",
    "best_forest_accuracy = 0\n",
    "best_forest_params = {}\n",
    "\n",
    "for n_estimators in [50, 100, 150]:  # Probar diferentes números de árboles\n",
    "    for max_depth in range(5, 16, 5):  # Probar profundidades de 5, 10, 15\n",
    "        for min_samples_split in [2, 5, 10]:  # Probar diferentes divisiones mínimas\n",
    "            forest_model = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                random_state=54321\n",
    "            )\n",
    "            forest_model.fit(features_train, target_train)\n",
    "            forest_predictions = forest_model.predict(features_valid)\n",
    "            forest_accuracy = accuracy_score(target_valid, forest_predictions)\n",
    "            \n",
    "            # Guardar la mejor configuración\n",
    "            if forest_accuracy > best_forest_accuracy:\n",
    "                best_forest_accuracy = forest_accuracy\n",
    "                best_forest_params = {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split\n",
    "                }\n",
    "\n",
    "print(f\"Mejor exactitud del Bosque Aleatorio: {best_forest_accuracy:.2f}\")\n",
    "print(f\"Mejores hiperparámetros del Bosque Aleatorio: {best_forest_params}\")\n",
    "\n",
    "# Modelo 3: Regresión logística\n",
    "# Ajustar hiperparámetros: C, solver, max_iter\n",
    "best_logistic_accuracy = 0\n",
    "best_logistic_params = {}\n",
    "\n",
    "for C in [0.01, 0.1, 1, 10, 100]:  # Probar diferentes valores de regularización\n",
    "    for solver in ['liblinear', 'lbfgs']:  # Probar diferentes algoritmos de optimización\n",
    "        for max_iter in [100, 200, 300]:  # Probar diferentes iteraciones máximas\n",
    "            logistic_model = LogisticRegression(\n",
    "                C=C,\n",
    "                solver=solver,\n",
    "                max_iter=max_iter,\n",
    "                random_state=54321\n",
    "            )\n",
    "            logistic_model.fit(features_train, target_train)\n",
    "            logistic_predictions = logistic_model.predict(features_valid)\n",
    "            logistic_accuracy = accuracy_score(target_valid, logistic_predictions)\n",
    "            \n",
    "            # Guardar la mejor configuración\n",
    "            if logistic_accuracy > best_logistic_accuracy:\n",
    "                best_logistic_accuracy = logistic_accuracy\n",
    "                best_logistic_params = {\n",
    "                    'C': C,\n",
    "                    'solver': solver,\n",
    "                    'max_iter': max_iter\n",
    "                }\n",
    "\n",
    "print(f\"Mejor exactitud de la Regresión Logística: {best_logistic_accuracy:.2f}\")\n",
    "print(f\"Mejores hiperparámetros de la Regresión Logística: {best_logistic_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Bosque Aleatorio es el mejor modelo, ya que tiene la mayor exactitud (80%). \n",
    "\n",
    "Este modelo supera el umbral de exactitud requerido (0.75), lo que indica que es adecuado para recomendar los planes Smart o Ultra a los clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del mejor modelo en el conjunto de prueba: 0.8102643856920684\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar el mejor modelo (Bosque Aleatorio)\n",
    "best_model = RandomForestClassifier(random_state=54321)\n",
    "best_model.fit(features_train, target_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "test_predictions = best_model.predict(features_test)\n",
    "test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "print(\"Exactitud del mejor modelo en el conjunto de prueba:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo de referencia: 0.7247278382581649\n",
      "El modelo supera la prueba de cordura.\n"
     ]
    }
   ],
   "source": [
    "#Prueba de cordura\n",
    "# Modelo de referencia: Para predicir la clase mayoritaria (Smart - 0)\n",
    "baseline_predictions = [0] * len(target_test)\n",
    "baseline_accuracy = accuracy_score(target_test, baseline_predictions)\n",
    "print(\"Exactitud del modelo de referencia:\", baseline_accuracy)\n",
    "\n",
    "# Comparar con el mejor modelo\n",
    "if test_accuracy > baseline_accuracy:\n",
    "    print(\"El modelo supera la prueba de cordura.\")\n",
    "else:\n",
    "    print(\"El modelo no supera la prueba de cordura.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de Bosque Aleatorio supera la prueba de cordura, ya que su exactitud (81,03%) es mayor que la del modelo de referencia (72.47%). El modelo de referencia o baseline en este caso es un modelo que siempre predice la clase mayoritaria (en este caso el plan Smart - 0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSIÓN:\n",
    "\n",
    "El Bosque Aleatorio es el mejor modelo, ya que tiene la mayor exactitud tanto en el conjunto de validación (78.23%) como en el conjunto de prueba (81.03%).\n",
    "\n",
    "Este modelo supera el umbral de exactitud requerido (0.75), lo que indica que es adecuado para recomendar los planes Smart o Ultra a los clientes de Megaline basándose en su comportamiento mensual."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 790,
    "start_time": "2025-03-06T18:42:52.585Z"
   },
   {
    "duration": 10,
    "start_time": "2025-03-06T18:46:48.401Z"
   },
   {
    "duration": 339,
    "start_time": "2025-03-06T18:47:27.272Z"
   },
   {
    "duration": 316,
    "start_time": "2025-03-06T18:49:11.566Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-06T18:51:33.751Z"
   },
   {
    "duration": 162,
    "start_time": "2025-03-07T15:33:47.308Z"
   },
   {
    "duration": 783,
    "start_time": "2025-03-07T15:33:56.661Z"
   },
   {
    "duration": 23,
    "start_time": "2025-03-07T15:33:59.782Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-07T15:37:22.197Z"
   },
   {
    "duration": 797,
    "start_time": "2025-03-07T15:53:45.630Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-07T15:53:46.429Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-07T15:53:46.448Z"
   },
   {
    "duration": 6901,
    "start_time": "2025-03-07T15:53:46.457Z"
   },
   {
    "duration": 318,
    "start_time": "2025-03-07T15:53:53.361Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-07T15:53:53.681Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
